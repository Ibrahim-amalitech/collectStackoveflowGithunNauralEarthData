{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8461f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://dev.azure.com/AmaliTech-BI/StackOverflow/_git/StackOverflow\n",
      "   6968758..68d8d0a  Ibrahim_branch -> Ibrahim_branch\n"
     ]
    }
   ],
   "source": [
    "# !git add getStackOverFlowDataToSql.ipynb\n",
    "# !git reset getStackOverFlowDataToSql.ipynb\n",
    "# !git checkout -b Ibrahim_branch\n",
    "# !git add NaturalEarthToAzureSQL.ipynb\n",
    "# !git commit -m \"Added script for collecting and updating Geographical data\"\n",
    "# !git push origin Ibrahim_branch\n",
    "\n",
    "\n",
    "# !git checkout Ibrahim_branch  \n",
    "# !git pull origin Ibrahim_branch  \n",
    "# !git checkout Ibrahim_branch\n",
    "# !git merge Ibrahim_branch  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "332ad800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import pyodbc  \n",
    "import pandas as pd\n",
    "import ogr\n",
    "import shutil\n",
    "\n",
    "# from airflow import DAG\n",
    "# from airflow.hooks.base import BaseHook\n",
    "# from airflow.providers.odbc.hooks.odbc import OdbcHook\n",
    "# from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "import csv,sys\n",
    "\n",
    "# from airflow.models import BaseOperator\n",
    "from sqlalchemy import create_engine,text,String  \n",
    "# from airflow.utils.dates import days_ago\n",
    "# from airflow.providers.mssql.hooks.mssql import MsSqlHook\n",
    "import pyodbc\n",
    "# from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7446530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table structure for 'ne_10m_admin_1_states_provinces' does not match. Skipping update.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IbrahimOsuman\\AppData\\Local\\Temp\\ipykernel_6020\\2561155753.py:72: DtypeWarning: Columns (104,105,106,108,110,114,115,116,118,119,122,123,124,125,126,127,128,129,131,132,133,135,136) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(BytesIO(csv_content), encoding='utf-8-sig')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table structure for 'ne_10m_populated_places' does not match. Skipping update.\n",
      "Table structure for 'ne_10m_admin_0_countries_arg' does not match. Skipping update.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text, MetaData, Unicode, String\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import zipfile\n",
    "from sqlalchemy import inspect  \n",
    "\n",
    "#  connection parameters\n",
    "server_name = \"\"  \n",
    "database_name = \"\"  \n",
    "username = \"\"  \n",
    "password = \"\"  \n",
    "\n",
    "\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f\"mssql+pyodbc://{username}:{password}@{server_name}/{database_name}?driver=ODBC+Driver+17+for+SQL+Server&charset=utf8mb4\"\n",
    "\n",
    "# Create an SQL Server engine\n",
    "engine = create_engine(connection_string, connect_args={'convert_unicode': True})\n",
    "\n",
    "# Define the language columns\n",
    "language_columns = ['NAME_AR']\n",
    "\n",
    "def convert_shape_to_csv(gdf):\n",
    "    csv_buffer = BytesIO()\n",
    "    gdf.to_csv(csv_buffer, index=False, encoding='utf-8-sig', mode='wb')\n",
    "    csv_buffer.seek(0)\n",
    "    return csv_buffer\n",
    "\n",
    "def compare_table_structure(engine, table_name, df):\n",
    "    inspector = inspect(engine)  # Use the inspect function\n",
    "    \n",
    "    existing_columns = inspector.get_columns(table_name)  # Get existing columns\n",
    "    \n",
    "    reflected_columns = [col['name'] for col in existing_columns]\n",
    "    df_columns = df.columns\n",
    "    \n",
    "    return set(reflected_columns) == set(df_columns)\n",
    "\n",
    "def transfer_geographical_data_to_sql():\n",
    "    file_urls = [\n",
    "        \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip\",\n",
    "        \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_populated_places.zip\",\n",
    "        \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries_arg.zip\"\n",
    "    ]\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    \n",
    "    arabic_name_column = \"NAME_AR\"\n",
    "    \n",
    "    for file_url in file_urls:\n",
    "        response = requests.get(file_url, headers=headers, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "                extracted_folder = os.path.splitext(os.path.basename(file_url))[0]\n",
    "                zip_ref.extractall(extracted_folder)\n",
    "            \n",
    "            shapefile_path = os.path.join(extracted_folder, f\"{os.path.splitext(os.path.basename(file_url))[0]}.shp\")\n",
    "            \n",
    "            gdf = gpd.read_file(shapefile_path)\n",
    "            csv_buffer = convert_shape_to_csv(gdf)\n",
    "            csv_content = csv_buffer.getvalue()\n",
    "            df = pd.read_csv(BytesIO(csv_content), encoding='utf-8-sig')\n",
    "                # Define data types for columns\n",
    "            def set_data_type(column):\n",
    "                if column in language_columns:\n",
    "                    return Unicode(length='max', collation='SQL_Latin1_General_CP1_CI_AS')\n",
    "                elif isinstance(df[column].dtype, sqlalchemy.types.TypeEngine):\n",
    "                    return df[column].dtype\n",
    "                else:\n",
    "                    return String(collation='SQL_Latin1_General_CP1_CI_AS')\n",
    "            \n",
    "            table_name = os.path.splitext(os.path.basename(file_url))[0]\n",
    "            \n",
    "            with engine.connect() as conn:\n",
    "                if not compare_table_structure(engine, table_name, df):\n",
    "                    print(f\"Table structure for '{table_name}' does not match. Skipping update.\")\n",
    "                    continue\n",
    "                \n",
    "                conn.execute(text(f\"DROP TABLE IF EXISTS {table_name}\"))\n",
    "                \n",
    "            dtype = {col: set_data_type(col) for col in df.columns}\n",
    "            \n",
    "            df.to_sql(table_name, engine, if_exists='replace', index=False,dtype=dtype)\n",
    "            print(f\"Table '{table_name}' updated or replaced.\")\n",
    "\n",
    "# Call the function\n",
    "transfer_geographical_data_to_sql()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17820b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of CSV content:\n",
      "  ADM0_A3_AR       featurecla  scalerank  LABELRANK SOVEREIGNT SOV_A3  \\\n",
      "0        IDN  Admin-0 country          0          2  Indonesia    IDN   \n",
      "1        MYS  Admin-0 country          0          3   Malaysia    MYS   \n",
      "2        CHL  Admin-0 country          0          2      Chile    CHL   \n",
      "3        BOL  Admin-0 country          0          3    Bolivia    BOL   \n",
      "4        PER  Admin-0 country          0          2       Peru    PER   \n",
      "\n",
      "   ADM0_DIF  LEVEL               TYPE  TLC  ... FCLASS_TR FCLASS_ID  \\\n",
      "0         0      2  Sovereign country  1.0  ...       NaN       NaN   \n",
      "1         0      2  Sovereign country  1.0  ...       NaN       NaN   \n",
      "2         0      2  Sovereign country  1.0  ...       NaN       NaN   \n",
      "3         0      2  Sovereign country  1.0  ...       NaN       NaN   \n",
      "4         0      2  Sovereign country  1.0  ...       NaN       NaN   \n",
      "\n",
      "   FCLASS_PL FCLASS_GR FCLASS_IT  FCLASS_NL FCLASS_SE FCLASS_BD  FCLASS_UA  \\\n",
      "0        NaN       NaN       NaN        NaN       NaN       NaN        NaN   \n",
      "1        NaN       NaN       NaN        NaN       NaN       NaN        NaN   \n",
      "2        NaN       NaN       NaN        NaN       NaN       NaN        NaN   \n",
      "3        NaN       NaN       NaN        NaN       NaN       NaN        NaN   \n",
      "4        NaN       NaN       NaN        NaN       NaN       NaN        NaN   \n",
      "\n",
      "                                            geometry  \n",
      "0  MULTIPOLYGON (((117.70360790395524 4.163414542...  \n",
      "1  MULTIPOLYGON (((117.70360790395524 4.163414542...  \n",
      "2  MULTIPOLYGON (((-69.51008875199994 -17.5065881...  \n",
      "3  POLYGON ((-69.51008875199994 -17.5065881979999...  \n",
      "4  MULTIPOLYGON (((-69.51008875199994 -17.5065881...  \n",
      "\n",
      "[5 rows x 169 columns]\n"
     ]
    }
   ],
   "source": [
    "# from sqlalchemy import create_engine, String\n",
    "# from sqlalchemy.types import Unicode\n",
    "# from sqlalchemy.types import TypeEngine\n",
    "# from io import BytesIO\n",
    "# import sqlalchemy\n",
    "# import pymysql\n",
    "\n",
    "# def convert_shape_to_csv(gdf):\n",
    "#     csv_buffer = BytesIO()\n",
    "#     gdf.to_csv(csv_buffer, index=False, encoding='utf-8-sig', mode='wb')\n",
    "#     csv_buffer.seek(0)\n",
    "#     return csv_buffer\n",
    "\n",
    "# def transfer_geographical_data_to_sql():\n",
    "# #     file_urls = [\n",
    "# #         \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip\",\n",
    "# #         \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_populated_places.zip\",\n",
    "# #         \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries_arg.zip\"\n",
    "# #     ]\n",
    "#     file_urls = [\n",
    "#         \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries_arg.zip\",\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     language_columns = ['NAME_AR', ]\n",
    "    \n",
    "# #     # Define your connection parameters\n",
    "# #     server_name = \"data-science.ck0dwzfc16xf.eu-west-1.rds.amazonaws.com\"  # Replace with your server name\n",
    "# #     database_name = \"TrendStreamDB\"  # Replace with your database name\n",
    "# #     username = \"admin\"  # Replace with your username\n",
    "# #     password = \"dAta$ci3nCe\"  # Replace with your password\n",
    "\n",
    "    \n",
    "    \n",
    "#     headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    \n",
    "#     arabic_name_column =\"NAME_AR\"\n",
    "    \n",
    "#     # Define data types for columns\n",
    "#     def set_data_type(column):\n",
    "#         if column in language_columns:\n",
    "#             return Unicode(length='max', collation='SQL_Latin1_General_CP1_CI_AS')\n",
    "#         elif isinstance(df[column].dtype, sqlalchemy.types.TypeEngine):\n",
    "#             return df[column].dtype\n",
    "#         else:\n",
    "#             return String(collation='SQL_Latin1_General_CP1_CI_AS')\n",
    "\n",
    "#     for file_url in file_urls:\n",
    "#         response = requests.get(file_url, headers=headers, stream=True)\n",
    "#         if response.status_code == 200:\n",
    "#             with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "#                 extracted_folder = os.path.splitext(os.path.basename(file_url))[0]\n",
    "#                 zip_ref.extractall(extracted_folder)\n",
    "            \n",
    "#             # Get the shapefile path from the extracted folder\n",
    "#             shapefile_path = os.path.join(extracted_folder, f\"{os.path.splitext(os.path.basename(file_url))[0]}.shp\")\n",
    "            \n",
    "#             # Convert the shapefile to GeoDataFrame\n",
    "#             gdf = gpd.read_file(shapefile_path)\n",
    "            \n",
    "#             # Convert the GeoDataFrame to CSV\n",
    "#             csv_buffer = convert_shape_to_csv(gdf)\n",
    "            \n",
    "#             # Read and print the first few lines of the CSV content\n",
    "#             csv_content = csv_buffer.getvalue()\n",
    "#             df = pd.read_csv(BytesIO(csv_content), encoding='utf-8-sig')\n",
    "#             print(f\"First few lines of CSV content:\\n{df.head()}\")\n",
    "            \n",
    "#             # Save user data to SQL Server\n",
    "#             server_name = \"localhost\\\\MYSQL2019\"\n",
    "#             database_name = \"Data\"\n",
    "#             username = \"connect\"\n",
    "#             password = \"O4i4m9b8g##\"\n",
    "            \n",
    "#             # Create the connection string\n",
    "#             connection_string = f\"mssql+pyodbc://{username}:{password}@{server_name}/{database_name}?driver=ODBC+Driver+17+for+SQL+Server&charset=utf8mb4\"\n",
    "\n",
    "#             # Create an SQL Server engine\n",
    "#             engine = create_engine(connection_string,connect_args={'convert_unicode': True})\n",
    "            \n",
    "#             # Get the table name from the file URL\n",
    "#             table_name = os.path.splitext(os.path.basename(file_url))[0]\n",
    "            \n",
    "#             # Drop the table if it already exists\n",
    "#             drop_table_query = text(f\"IF OBJECT_ID('{table_name}', 'U') IS NOT NULL DROP TABLE {table_name}\")\n",
    "\n",
    "#             with engine.connect() as conn:\n",
    "#                 conn.execute(drop_table_query)\n",
    "                \n",
    "#             # Define data types with collation for specified columns\n",
    "            \n",
    "# #             dtype = {\n",
    "# #                     col: Unicode(length='max', collation='SQL_Latin1_General_CP1_CI_AS') \n",
    "# #                     if col in language_columns \n",
    "# #                     else String(collation='SQL_Latin1_General_CP1_CI_AS') \n",
    "# #                     for col in df.columns\n",
    "# #                 }\n",
    "# #             print(dtype)\n",
    "# #             dtype = {col: set_data_type(col) for col in df.columns}\n",
    "# #             dtype = df.apply(lambda col: String(collation='SQL_Latin1_General_CP1_CI_AS') if col.dtype == 'object' else col.dtype)\n",
    "            \n",
    "#             # Save the DataFrame to the SQL Server table\n",
    "#             df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "\n",
    "# # Call the function\n",
    "# transfer_geographical_data_to_sql()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22aabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
